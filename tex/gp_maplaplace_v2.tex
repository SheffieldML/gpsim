\documentclass[10pt]{article}

\textwidth16.5cm
\textheight25.8cm
\topmargin-2.0cm
\footskip1.3cm
\oddsidemargin-0.3cm
\evensidemargin-0.3cm
\setlength{\parindent}{0.3cm}
\usepackage{bm} %boldmath
\usepackage{graphicx}
\bibliographystyle{apalike}
\usepackage[round]{natbib}
\bibsep0.0cm

\begin{document}

\title{Laplace approximation calculations}

\author{Magnus Rattray}

\maketitle

\section*{Notation}

We use lower case for functions, e.g. $k$ and $w$, and upper case 
for the corresponding matrices in the numerical implementation,
e.g. $K$ and $W$.

\section{Gradient and Hessian}

\begin{equation}
\frac{dx_j}{dt} = B_j + g(f(t),\theta_j) - D_j x_j \ .
\end{equation}
We set the initial conditions $x_j(0)=B_j/D_j$ so that,
\begin{equation}
x_j(t) = \frac{B_j}{D_j} + \mathrm{e}^{-D_j t}\int_{0}^t \mathrm{d}u \, \, g(f(u),\theta_j) \, \mathrm{e}^{D_j u} \ .
\end{equation}
The log-likelihood of data $D = \{y_{ij}\}$ for gene $j$ at times $t_i$ is,
\begin{equation}
p(D|f,\{B_j,\theta_j,D_j\}) = -\frac{1}{2}\sum_{i=1}^T \sum_{j=1}^m \left[\lambda_{ij} 
\left( x_j(t_i) - y_{ij} \right)^2 + \log\left(\lambda_{ij}\right)\right] - \frac{mT}{2}\log(2\pi) \ .
\end{equation}
The functional
gradient of the log-likelihood with respect to $f$ is,
\begin{equation}
\frac{\delta \log p(D|f)}{\delta f} = -\sum_{i=1}^T \Theta(t_i - t)\sum_{j=1}^m \lambda_{ij} 
(x_j(t_i) - y_{ij})\frac{\partial g(f,\theta_j)}{\partial
  f} \mathrm{e}^{-D_j(t_i-t)}dt
\end{equation}
The -ve Hessian of the log-likelihood with respect to $f$ is,
\begin{eqnarray}
& & w = - \frac{\delta^2 \log p(D|f)}{\delta f^2} = \sum_{i=1}^T\Theta(t_i - t)\sum_{j=1}^m \lambda_{ij} 
(x_j(t_i) - y_{ij}) g''(f(t),\theta_j)\mathrm{e}^{-D_j(t_i-t)}dt 
\nonumber \\ & & \quad + \: \sum_{i=1}^T \Theta(t_i-t)\Theta(t_i-s)\sum_{j=1}^m \lambda_{ij} 
g'(f(t),\theta_j) g'(f(s),\theta_j) \mathrm{e}^{-D_j(2t_i-t-s)}dtds 
\end{eqnarray}
where $g'(f)=\partial g/\partial f$ and $g''(f)=\partial^2 g/\partial f$. The gradient and Hessian of the
unnormalised log posterior are,
\begin{eqnarray}
\frac{\delta \log p(f|D)}{\delta f} = \frac{\delta \log p(D|f)}{\delta
  f} - k^{-1}f \\
\frac{\delta^2 \log p(f|D)}{\delta f^2} = -w-k^{-1}
\end{eqnarray}

\subsection{Numerical implementation}

We discretize time $t_k$ with $\tau = t_{k}-t_{k-1}$ constant. We
write $\bm f=[f_k]$ to be the vector realisation of the function
$f$. The gradient of the log-likelihood is then given by,
\begin{equation}
\nabla_k \log p(D|\bm f) = \frac{\partial \log p(D|\bm f)}{\partial f_k} = -\tau\sum_{i=1}^T \Theta(t_i - t_k)\sum_{j=1}^m \lambda_{ij} 
(x_j(t_i) - y_{ij})\partial g'(f_k,\theta_j) \mathrm{e}^{-D_j(t_i-t_k)}
\end{equation}
and the -ve Hessian of the log-likelihood is,
\begin{eqnarray}
& & W_{kl} = -\nabla_k \nabla_l \log p(D|\bm f) = \delta_{kl} \tau \sum_{i=1}^T
\Theta(t_i-t_k)\sum_{j=1}^m \lambda_{ij}(x_j(t_i) - y_{ij})
g''(f_k,\theta_j)
 \mathrm{e}^{-D_j(t_i-t_k)}     \nonumber \\
& & \quad + \: \tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\sum_{j=1}^m \lambda_{ij} 
g'(f_k,\theta_j) g'(f_l,\theta_j)
\mathrm{e}^{-D_j(2t_i-t_k-t_l)} 
\end{eqnarray}
where $\delta_{kl}$ is the Kronecker delta.

\section{MAP solution and Laplace approximation}

The gradient and Hessian of the unnormalised log posterior $\Psi(\bm f) = \log p(D|\bm f) + \log p(\bm f)$ are,
\begin{eqnarray}
\nabla \Psi(\bm f) & = & \nabla \log p(D|\bm f) -K^{-1}{\bm f}
\label{eqn_MAP} \ , \\
\nabla\nabla \Psi(\bm f) & = & - (W + K^{-1}) \ .
\end{eqnarray}
We use this matrix to find the Newton direction for optimisation,
\begin{equation}
	\Delta f = -\eta (W+K^{-1})^{-1} (\nabla L - K^{-1}\bm f)
\end{equation}
where $L=\log p(D|\bm f)$. 
Doing a full Newton up-date one finds,
\begin{equation}
  {\bm f} \leftarrow (W+K^{-1})^{-1}(W\bm{f} + \nabla L)
\end{equation}
which converges quickly. The matrix inversion lemma can be used to
speed up the inversion of $I+KW$, since $W$ can be written as a sum of
outer-products, but I haven't bothered with that yet. The Laplace approximation to the log
marginal likelihood is (ignoring terms that do not involve model parameters),
\begin{eqnarray}
\log p(D|\{B_j,\theta_j,D_j\},\gamma) & \simeq & \log p(D|\hat{\bm f}) -
\mbox{$\frac{1}{2}$}\hat{\bm f}^T K^{-1} \hat{\bm f} - \mbox{$\frac{1}{2}$}\log|W+K^{-1}| 
- \mbox{$\frac{1}{2}$}\log|K| \nonumber \\
& = & \log p(D|\hat{\bm f}) - \mbox{$\frac{1}{2}$}\hat{\bm f}^T K^{-1}
\hat{\bm f} - \mbox{$\frac{1}{2}$}\log|I+KW| 
\label{eqn_marginal}
\end{eqnarray}
where $\hat{\bm f}$ is the MAP solution and $\gamma$ are the kernel parameters.

\subsection{Linear case}

We first check the results for the simplest case,
\begin{equation}
g(f,S_j) = S_j \, f \ , \qquad k(t,t')  =
\exp\left(\frac{-\gamma(t-t')^2}{2} \right) \ .
\end{equation}
We will treat the noise as known and ignore noise propagation for
now, so $\lambda_{ij}$ does not have to be estimated. The gradients we have to work out are,
\begin{eqnarray*}
\frac{\partial x_j(t)}{\partial B_j} & = & \frac{1}{D_j} \\
\frac{\partial x_j(t)}{\partial S_j} & = & \mathrm{e}^{-D_j t}\int_{0}^t \mathrm{d}u 
\, f(u) \mathrm{e}^{D_j u} = \frac{x_j(t)-B_j/D_j}{S_j} \\
\frac{\partial x_j(t)}{\partial D_j} & = & -\frac{B_j}{D_j^2} + S_j \, \mathrm{e}^{-D_j t}\int_{0}^t \mathrm{d}u 
\, f(u)(u - t)\mathrm{e}^{D_j u}
\end{eqnarray*}
and then for any parameter $z_j$,
\begin{equation}
\frac{\partial \log p(D|f)}{\partial z_j} = -\sum_{i=1}^T \lambda_{ij} 
(x_j(t_i) - y_{ij})\frac{\partial x_j(t_i)}{\partial z_j} \\
\end{equation}
The Hessian has no dependence on $f$, it only depends on the
parameters $D_j$ and $S_j$,
\begin{equation}
W_{kl} = \tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\sum_{j=1}^m \lambda_{ij} S_j^2
\mathrm{e}^{-D_j(2t_i-t_k-t_l)}\ .
\end{equation}
The gradients with respect to these parameters are,
\begin{eqnarray*}
\frac{\partial W_{kl}}{\partial S_j} & = & 2\tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\lambda_{ij} S_j
\mathrm{e}^{-D_j(2t_i-t_k-t_l)} \\
\frac{\partial W_{kl}}{\partial D_j} & = & -\tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\lambda_{ij} S_j^2
(2t_i-t_k-t_l) \mathrm{e}^{-D_j(2t_i-t_k-t_l)} 
\end{eqnarray*}
and then for any parameter $z$,
\begin{equation}
\frac{\partial}{\partial z}\log|I+KW|  = \mbox{tr}\left[
  (I+KW)^{-1}K\frac{\partial W}{\partial z}\right]
\end{equation}
The gradient of the kernel with respect to its parameter is,
\begin{equation}
\frac{\partial K_{kl}}{\partial \gamma} = -\mbox{$\frac{1}{2}$}(t_k-t_l)^2K_{kl}
\end{equation}
and we have,
\begin{eqnarray*}
\frac{\partial}{\partial \gamma}\log|I+KW| & = & \mbox{tr}\left[
  (I+KW)^{-1}W\frac{\partial K}{\partial \gamma}\right] \ , \\
\frac{\partial K^{-1}}{\partial \gamma} & = & -K^{-1}\frac{\partial
  K}{\partial \gamma}K^{-1} \ .
\end{eqnarray*}
So we find,
\begin{equation}
\frac{\partial \log p(D|\gamma)}{\partial \gamma} =
-\mbox{$\frac{1}{2}$}\hat{\bm f}^T K^{-1}\frac{\partial K}{\partial \gamma} K^{-1}\hat{\bm f} - 
\mbox{$\frac{1}{2}$}\mbox{tr}\left((W^{-1}+K)^{-1}\frac{\partial
  K}{\partial \gamma}\right) 
\end{equation}

\subsection{Constraining the TF concentrations to be positive}
We constrain the function to be positive,
\begin{equation}
g(f,S_j) = S_j \, \mathrm{e}^f \ , \qquad k(t,t')  =
\exp\left(\frac{-\gamma(t-t')^2}{2} \right) \ .
\end{equation}
I will only consider optimisation of $\bm f$ by MAP learning and the
kernel parameters by MAP-Laplace. The gradient and Hessian are,
\begin{eqnarray}
\nabla_k \log p(D|\bm f) = \frac{\partial \log p(D|\bm
  f)}{\partial f_k} = -\tau\sum_{i=1}^T \Theta(t_i - t_k)\sum_{j=1}^m \lambda_{ij} 
(x_j(t_i) - y_{ij})S_j \mathrm{e}^{f_k-D_j(t_i-t_k)} \\
W_{kl} = -\nabla_k \nabla_l \log p(D|\bm f) = \delta_{kl} \tau \sum_{i=1}^T
  \Theta(t_i-t_k)\sum_{j=1}^m \lambda_{ij} 
(x_j(t_i) - y_{ij})S_j \mathrm{e}^{f_k-D_j(t_i-t_k)}  \nonumber \\
+ \tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\sum_{j=1}^m \lambda_{ij} 
S_j^2 \mathrm{e}^{f_k+f_l-D_j(2t_i-t_k-t_l)} \nonumber \\
= -\delta_{kl}\nabla_k \log p(D|\bm f) + \tau^2\sum_{i=1}^T \Theta(t_i-t_k)\Theta(t_i-t_l)\sum_{j=1}^m \lambda_{ij} 
S_j^2 \mathrm{e}^{f_k+f_l-D_j(2t_i-t_k-t_l)} \label{eqn_hessian_pos} \ .
\end{eqnarray}
The term in the Hessian proportional to the gradient vanishes at the
MAP solution. As before the gradient of the kernel with respect to its parameter is,
\begin{equation}
\frac{\partial K_{kl}}{\partial \gamma} =
-\mbox{$\frac{1}{2}$}(t_k-t_l)^2K_{kl} \ .
\end{equation}
The gradient of the log-marginal with respect to $\gamma$ is,
\begin{eqnarray}
\frac{\partial \log p(D|\gamma)}{\partial \gamma}& = &
-\mbox{$\frac{1}{2}$}\hat{\bm f}^T K^{-1}\frac{\partial K}{\partial \gamma} K^{-1}\hat{\bm f} - 
\mbox{$\frac{1}{2}$}\mbox{tr}\left((W^{-1}+K)^{-1}\frac{\partial
  K}{\partial \gamma}\right) + \sum_k \frac{\partial \log p(D|\gamma)}{\partial \hat{f}_k}\frac{\hat{f}_k}{\partial \gamma}
\end{eqnarray}
where $\hat{\bm
  f}$ is the MAP solution and the final term is due to the implicit dependence of $\hat{\bm
  f}$ on $\gamma$. The only term that contributes to this is the final
  one in equation~(\ref{eqn_marginal}) which involves $W$. We find,
\begin{equation}
  \frac{\partial \log p(D|\gamma)}{\partial \hat{f}_k} =
  -\frac{1}{2}\mbox{tr}\left( (K^{-1}+W)^{-1}\frac{\partial
  W}{\partial \hat{f}_k}\right) 
\end{equation}
At the MAP solution we find,
\[
\frac{\partial W_{pq}}{\partial \hat{f}_k} =
(\delta_{kp}+\delta_{kq})W_{pq} \ .
\]
where we have used the self-consistent condition that the first term in
equation~(\ref{eqn_hessian_pos}) vanishes at the MAP solution. This
  then simplifies to,
\[
\frac{\partial \log p(D|\gamma)}{\partial \hat{f}_k} =
-((W+K^{-1})^{-1}W)_{kk} \ .
\]
>From equation~(\ref{eqn_hessian_pos}) we have the
self-consistent equation $\hat{\bm f}=K\nabla\log p(D|\hat{\bm f})$
and differentiating that we get,
\begin{equation}
\frac{\partial \hat{\bm f}}{\partial \gamma} = (W+K^{-1})^{-1}K^{-1}\frac{\partial
  K}{\partial \gamma} \nabla \log p(D|\hat{\bm f}) \ .
\end{equation}

\end{document}
