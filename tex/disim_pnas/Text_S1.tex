\documentclass{article}
\usepackage{amsmath}
%\usepackage{a4wide}

\title{Text S1 for ``Model-based transcription factor
  target identification with limited data''}


\author{Antti Honkela et al.}
\date{}

\newcommand{\erf}{\operatorname{erf}}

\oddsidemargin .5in    %   Note \oddsidemargin = \evensidemargin
\evensidemargin .5in
\marginparwidth 0.07 true in
%\marginparwidth 0.75 true in
%\topmargin 0 true pt           % Nominal distance from top of page to top of
%\topmargin 0.125in
\topmargin -0.625in
\addtolength{\headsep}{0.25in}
\textheight 9.0 true in       % Height of text (including footnotes & figures)
\textwidth 5.5 true in        % Width of text line.
\widowpenalty=10000
\clubpenalty=10000


\begin{document}

\maketitle

\section{Derivation of the Gaussian process model}

The linear system ODEs underlying our model is
\begin{align}
  \label{eq:translation_ode}
  \frac{df(t)}{dt} & = f(t) - \delta p(t) \\
  \label{eq:translation_ode}
  \frac{dm_j(t)}{dt} & = B_j + S_j p(t) - D_j m_j(t).
\end{align}
Assuming steady-state initial conditions $f(0) = 0$ and
$m_j(0) = B_j / D_j$, its solution is
\begin{align}
  \label{eq:protein_ode_sol}
  p(t) & = \exp(-\delta t) \int_0^t f(v) \exp(\delta v) \, dv \\
  \label{eq:mrna_ode_sol}
  m_j(t) & = \frac{B_j}{D_j} + S_j \exp(-D_j t) \int_0^t \exp(D_j
  u) \exp(-\delta u) \int_0^u f(v) \exp(\delta v) \, dv \, du.
\end{align}

Both of these are linear operators of $f(t)$.  Hence, placing a
Gaussian process prior on $f(t)$ implies a joint Gaussian process
model over all $(f(t), p(t), m_j(t))$~\cite{Rasmussen2006}.

This Gaussian process is completely characterised by its mean and
covariance functions.  Assuming $\mathrm{E}[f(t)] = 0$, the above
solutions (\ref{eq:protein_ode_sol})-(\ref{eq:mrna_ode_sol}) imply
$\mathrm{E}[p(t)] = 0$, $\mathrm{E}[m_j(t)] = B_j / D_j$.

What remains is to determine the covariance functions.  These can
be evaluated as expectations
\begin{equation}
  \label{eq:kernel_definition}
  k_{xy}(t,t') = \mathrm{E}[(x(t) - \mathrm{E}[x(t)])(y(t') - \mathrm{E}[y(t')])],
\end{equation}
where $x,y \in \{f, p, m_j\}$.  Assuming the squared exponential
covariance for $f(t)$,
\begin{equation}
  \label{eq:sqexp_kernel}
  k_{ff}(t, t') = a \exp\left( -\frac{(t-t')^2}{l^2} \right),
\end{equation}
all the required covariance functions can be derived in closed form
by repeated application of the identity
\begin{multline}
  \label{eq:gpsim_identity}
  \int_0^t \exp(D u) \erf(u/l + E)\,du =
  \frac{1}{D} \bigg[
  \exp(Dt) \erf(E + t/l) - \erf(E) \\
  + \exp\left(\left( \frac{Dl}{2}\right)^2 -E Dl \right)
  [ \erf(E - Dl/2) - \erf(E-Dl/2+t/l) ]
  \bigg].
\end{multline}

\subsection{Covariance function $k_{fp}$}

The covariance $k_{fp}$ is the same as the cross-covariance derived
in~\cite{Lawrence2007}:
\begin{equation}
  \begin{split}
    k_{fp}(t, t') &= \exp(-\delta t) \int_0^{t} \exp(\delta u) k_{ff}(u, t') du \\
    &= \frac{\sqrt{\pi}al}{2} \exp\left(\left(\frac{\delta l}{2}\right)^2 + \delta (t - t') \right)
    [\erf(\delta l / 2 + t/l) - \erf(\delta l / 2 + (t-t')/l)]
  \end{split}
\end{equation}

\subsection{Covariance function $k_{fm_j}$}

\begin{multline}
  k_{f m_j}(t, t') = S_j \exp(-D_j t') \int_0^{t'}
  \exp((D_j - \delta) u) \int_0^u \exp(\delta v) k_{ff}(t, v)\, dv\, du \\
  = S_j \frac{\sqrt{\pi} al}{2(\delta - D_j)} \exp(-(D_j+\delta) t')) \\
  \bigg(
  \exp\left(\left(\frac{D_j l}{2}\right)^2 + D_j t + \delta t' \right)
  [\erf(D_j l / 2 + t/l) - \erf(D_j l / 2 + (t-t')/l)] \\
  -
  \exp\left(\left(\frac{\delta l}{2}\right)^2 + \delta t + D_j t'\right)
  [\erf(\delta l / 2 + t/l) - \erf(\delta l / 2 + (t-t')/l)]
  \bigg)
\end{multline}

\subsection{Covariance function $k_{pp}$}

Again following~\cite{Lawrence2007},
\begin{align*}
  k_{p p}(t, t') &= \exp(-\delta (t +t')) \int_0^t \exp(\delta u)
  \int_0^{t'} \exp(\delta u') k_{ff}(u, u') du' du \\
  &= \frac{\sqrt{\pi}al}{4 \delta}
  \exp\left(\left(\frac{\delta l}{2}\right)^2 -\delta (t + t')\right) [ h(t', t) +
  h(t, t')],
\end{align*}
where
\begin{multline}
  \label{eq:gpsim_h}
  h(t', t) = 
  \bigg\{ \exp[2 \delta t ] \bigg[
       \erf\left(\left(\frac{\delta l}{2}\right) + \frac{t}{l} \right) - 
       \erf\left(\left(\frac{\delta l}{2}\right) + \frac{t-t'}{l} \right) \bigg] \\
  + \bigg[
       \erf\left(\left(\frac{\delta l}{2}\right) - \frac{t'}{l} \right) -
       \erf\left(\left(\frac{\delta l}{2}\right) \right) \bigg] \bigg\}.
\end{multline}

\subsection{Covariance function $k_{pm_j}$}

\begin{multline}
  \frac{k_{p m_j}(t, t')}{S_j \exp(-\delta t - D_j t')} = 
  \int_0^{t'} \exp((D_j - \delta) u') \int_0^t \exp(\delta v) \int_0^{u'}
  \exp(\delta v') k_{yy}(v, v')\, dv'\, dv\, du' \\
  = 
  \frac{\sqrt{\pi}al}{4\delta}\exp\left(\left(\frac{\delta l}{2}\right)^2\right)
  \bigg(
  \frac{2 \delta \exp(- D_j t' - \delta t)}{\delta^2 - D_j^2}
  [\erf(\delta l / 2 - t/l) - \erf(\delta l / 2)]\\
  +
  \frac{\exp(- \delta (t + t'))}{\delta - D_j}
  [2\erf(\delta l / 2) - \erf(\delta l / 2 - t'/l) - \erf(\delta l / 2 - t/l)] \\
  +
  \frac{\exp(\delta (t' - t))}{\delta + D_j}
  [\erf(\delta l / 2 + t'/l) - \erf(\delta l / 2 - (t-t')/l)]\\
  +
  \frac{\exp(\delta (t - t'))}{\delta - D_j}
  [\erf(\delta l / 2 + (t-t')/l) - \erf(\delta l / 2 + t/l)]
  \bigg) \\
  + \frac{\sqrt{\pi}l}{2(\delta^2 - D_j^2)}
  \exp\left(\left(\frac{D_j l}{2}\right)^2 - D_j t' - \delta t \right)
  \bigg(
  \erf(D_j l/2 - t'/l) - \erf(D_j l/2) \\
  + \exp((D_j + \delta) t) [\erf(D_j l/2 + t/l) - \erf(D_j l/2 + (t-t')/l)]
  \bigg)
\end{multline}

\subsection{Covariance function $k_{m_j m_k}$}

The final covariance between target genes is
\begin{multline}
  k_{m_j m_k}(t, t') = S_j S_k \exp(-D_j t - D_k t')
  \int_0^t \exp((D_j - \delta) u)
  \int_0^{t'} \exp((D_k - \delta) u') \\
  \int_0^u \exp(\delta v) \int_0^{u'} \exp(\delta v') k_{ff}(v, v') \, dv'\, dv\, du'\, du \\
  = \frac{\sqrt{\pi} a l S_j S_k}{2} \bigg(
  h_{jk}(t, t', \delta) + h_{kj}(t', t, \delta) 
  - h_{jk}(t, t', D_j) - h_{kj}(t', t, D_k)
  \bigg)
\end{multline}
where
\begin{multline}
  h_{jk}(t, t', D_x) = 
  \exp\left(\left(\frac{D_x l}{2}\right)^2\right)
  \frac{\exp(-D_x t - D_k t')}{(D_x + \delta) (D_j - \delta)}
  \bigg\{ 
   \\
   %\frac{(D_k + \delta)\exp((D_k-\delta) t') - 2\delta}{(D_k^2-\delta^2)}
  \left(\frac{\exp((D_k-\delta) t') - 1}{D_k-\delta} +
    \frac{1}{D_k + D_x} \right)
  [\erf(D_x l/2 - t/l) - \erf(D_x l/2)]
  \\
  + \frac{\exp((D_k+D_x)t')}{D_k+D_x}
  [\erf(D_x l/2 + t'/l)
  - \erf(D_x l/2 - (t-t')/l)]
  \bigg\} \\
\end{multline}


%\bibliographystyle{pnas}
%\bibliography{disim}

\begin{thebibliography}{1}

\bibitem{Rasmussen2006}
Rasmussen, CE, Williams, CKI
\newblock (2006) \emph{Gaussian Processes for Machine Learning}
\newblock (MIT Press).

\bibitem{Lawrence2007}
Lawrence, ND, Sanguinetti, G, Rattray, M
\newblock (2007) in \emph{Advances in Neural Information Processing Systems},
  eds{} Sch\"{o}lkopf, B, Platt, JC, Hofmann, T
\newblock (MIT Press, Cambridge, MA) Vol.{}~19, pp 785--792.

\end{thebibliography}


\end{document}
